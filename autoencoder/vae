{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vae","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]}},"cells":[{"metadata":{"id":"Ji_mBqRuHoes","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","from keras.layers import Lambda, Input, Dense\n","from keras.models import Model\n","from keras.datasets import mnist\n","from keras.losses import mse, binary_crossentropy\n","from keras.utils import plot_model\n","from keras.models import Sequential as sq\n","from keras import backend as K\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import argparse\n","import os\n","\n","def sampling(args):\n","    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n","    # Arguments:\n","        args (tensor): mean and log of variance of Q(z|X)\n","    # Returns:\n","        z (tensor): sampled latent vector\n","    \"\"\"\n","\n","    z_mean, z_log_var = args\n","#from keras import backend as K : layer 형태\n","    batch = K.shape(z_mean)[0]\n","    dim = K.int_shape(z_mean)[1]\n","\n","    # by default, random_normal has mean=0 and std=1.0\n","    # 정규화 하는 식\n","    epsilon = K.random_normal(shape=(batch, dim))\n","    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n","\n","\n","def plot_results(models,\n","                 data,\n","                 batch_size=128,\n","                 model_name=\"vae_mnist\"):\n","    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n","    # Arguments:\n","        models (tuple): encoder and decoder models\n","        data (tuple): test data and label\n","        batch_size (int): prediction batch size\n","        model_name (string): which model is using this function\n","    \"\"\"\n","\n","    encoder, decoder = models\n","    x_test, y_test = data\n","    os.makedirs(model_name, exist_ok=True)\n","\n","    filename = os.path.join(model_name, \"vae_mean.png\")\n","    # display a 2D plot of the digit classes in the latent space\n","    z_mean, _, _ = encoder.predict(x_test,\n","                                   batch_size=batch_size)\n","    plt.figure(figsize=(12, 10))\n","    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n","    plt.colorbar()\n","    plt.xlabel(\"z[0]\")\n","    plt.ylabel(\"z[1]\")\n","    plt.savefig(filename)\n","    plt.show()\n","\n","    filename = os.path.join(model_name, \"digits_over_latent.png\")\n","    # display a 30x30 2D manifold of digits\n","    n = 30\n","    digit_size = 28\n","    figure = np.zeros((digit_size * n, digit_size * n))\n","    # linearly spaced coordinates corresponding to the 2D plot\n","    # of digit classes in the latent space\n","    grid_x = np.linspace(-4, 4, n)\n","    grid_y = np.linspace(-4, 4, n)[::-1]\n","\n","    for i, yi in enumerate(grid_y):\n","        for j, xi in enumerate(grid_x):\n","            z_sample = np.array([[xi, yi]])\n","            x_decoded = decoder.predict(z_sample)\n","            digit = x_decoded[0].reshape(digit_size, digit_size)\n","            figure[i * digit_size: (i + 1) * digit_size,\n","                   j * digit_size: (j + 1) * digit_size] = digit\n","\n","    plt.figure(figsize=(10, 10))\n","    start_range = digit_size // 2\n","    end_range = n * digit_size + start_range + 1\n","    pixel_range = np.arange(start_range, end_range, digit_size)\n","    sample_range_x = np.round(grid_x, 1)\n","    sample_range_y = np.round(grid_y, 1)\n","    plt.xticks(pixel_range, sample_range_x)\n","    plt.yticks(pixel_range, sample_range_y)\n","    plt.xlabel(\"z[0]\")\n","    plt.ylabel(\"z[1]\")\n","    plt.imshow(figure, cmap='Greys_r')\n","    plt.savefig(filename)\n","    plt.show()\n","\n","\n","\n","\n","\n","# MNIST dataset data set 만들기\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","image_size = x_train.shape[1]\n","#28*28\n","original_dim = image_size * image_size\n","\n","# [a,28,28,]->[a->784] , [-1 : 전부 받는다], np 형태로 바꿔 주기 위해서 np.reshape를 해줌\n","x_train = np.reshape(x_train, [-1, original_dim])\n","x_test = np.reshape(x_test, [-1, original_dim])\n","x_train = x_train.astype('float32') / 255\n","x_test = x_test.astype('float32') / 255\n","\n","# network parameters\n","# , 뒤는 제한 없이 받겠다.\n","input_shape = (original_dim, )\n","intermediate_dim = 512\n","latent_dim = 2\n","epochs = 50\n","batch_size = 128\n","\n","\n","# VAE model = encoder + decoder\n","# build encoder model\n","#784->512->2'\n","\n","\n","# inputs 은 입력을 받는 layer임을 Input을 통해서 알려줌. I i 차이\n","# 아래는 layer을 선언해줌.\n","inputs = Input(shape=input_shape, name='encoder_input')\n","x = Dense(intermediate_dim, activation='relu')(inputs)\n","# latent_dim = 2, 2차원에 대한 z_man, z_log_var를 가지고 싶어서.\n","z_mean = Dense(latent_dim, name='z_mean')(x)\n","z_log_var = Dense(latent_dim, name='z_log_var')(x)\n","\n","# use reparameterization trick to push the sampling out as input\n","# note that \"output_shape\" isn't necessary with the TensorFlow backend\n","#Lambda로 포장을 하면은 배열 입력 배열 반환이 된다.\n","z = Lambda(sampling, output_shape=(latent_dim, ), name='z')([z_mean, z_log_var])\n","'''\n","We can use these parameters to sample new similar points from the latent space:\n","'''\n","\n","# instantiate encoder model\n","\n","# input\n","encoder = Model(inputs,[z_mean, z_log_var, z], name='encoder')\n","\n","\n","# model.summary() prints a summary representation of your model.\n","# 모델에 대한 정보를 출력 해준다.\n","encoder.summary()\n","#plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n","\n","# build decoder model, decoder을 위한 구조, 위는 autoencoder 과정\n","latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n","x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n","outputs = Dense(original_dim, activation='sigmoid')(x)\n","\n","# instantiate decoder model\n","decoder = Model(latent_inputs, outputs, name='decoder')\n","decoder.summary()\n","#plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n","\n","# instantiate VAE model\n","outputs = decoder(encoder(inputs)[2])\n","vae = Model(inputs, outputs, name='vae_mlp')\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    help_ = \"Load h5 model trained weights\"\n","    parser.add_argument(\"-w\", \"--weights\", help=help_)\n","    help_ = \"Use mse loss instead of binary cross entropy (default)\"\n","    parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true')\n","    args = parser.parse_args()\n","    models = (encoder, decoder)\n","    data = (x_test, y_test)\n","\n","    # VAE loss = mse_loss or xent_loss + kl_loss\n","    if args.mse:\n","        reconstruction_loss = mse(inputs, outputs)\n","    else:\n","        reconstruction_loss = binary_crossentropy(inputs,outputs)\n","\n","    reconstruction_loss *= original_dim\n","    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n","    kl_loss = K.sum(kl_loss, axis=-1)\n","    kl_loss *= -0.5\n","    vae_loss = K.mean(reconstruction_loss + kl_loss)\n","    vae.add_loss(vae_loss)\n","    vae.compile(optimizer='adam')\n","    vae.summary()\n","    #plot_model(vae, to_file='vae_mlp.png')\n","\n","    if args.weights:\n","        vae = vae.load_weights(args.weights)\n","    else:\n","        # train the autoencoder\n","        vae.fit(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None))\n","        vae.save_weights('vae_mlp_mnist.h5')\n","\n","    plot_results(models, data, batch_size=batch_size, model_name=\"vae_mlp\")"],"execution_count":0,"outputs":[]}]}